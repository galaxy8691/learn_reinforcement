{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AdditionNet, self).__init__()\n",
    "        # 简单的三层网络\n",
    "        self.fc1 = nn.Linear(6, 256)  # 输入层\n",
    "        self.fc2 = nn.Linear(256, 1024) # 隐藏层\n",
    "        self.fc3 = nn.Linear(1024, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 256)\n",
    "        self.fc5 = nn.Linear(256, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.softmax(self.fc5(x), dim=1)  # 使用softmax确保两个输出和为1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_log(a, b, c, max_value=10000):\n",
    "    \"\"\"将数值分解为整数部分和小数部分，根据数量级设置不同精度\"\"\"\n",
    "    # 确定小数保留位数\n",
    "    if max_value <= 10:\n",
    "        decimal_places = 1\n",
    "    elif max_value <= 100:\n",
    "        decimal_places = 2\n",
    "    elif max_value <= 1000:\n",
    "        decimal_places = 3\n",
    "    else:\n",
    "        decimal_places = 4\n",
    "    \n",
    "    # 截断到指定小数位\n",
    "    factor = 10**decimal_places\n",
    "    a_truncated = math.floor(a * factor) / factor\n",
    "    b_truncated = math.floor(b * factor) / factor\n",
    "    c_truncated = math.floor(c * factor) / factor\n",
    "    \n",
    "    # 分离整数和小数部分\n",
    "    a_int, a_dec = int(a_truncated), a_truncated - int(a_truncated)\n",
    "    b_int, b_dec = int(b_truncated), b_truncated - int(b_truncated)\n",
    "    c_int, c_dec = int(c_truncated), c_truncated - int(c_truncated)\n",
    "    \n",
    "    # 使用log1p归一化整数部分\n",
    "    max_log = np.log1p(max_value * 2)\n",
    "    a_int_norm = np.log1p(a_int) / max_log\n",
    "    b_int_norm = np.log1p(b_int) / max_log\n",
    "    c_int_norm = np.log1p(c_int) / max_log\n",
    "    \n",
    "    # 小数部分只保留指定精度的位数，并放大\n",
    "    decimal_scale = 10**decimal_places\n",
    "    a_dec_scaled = int(a_dec * decimal_scale)\n",
    "    print(a_dec_scaled)\n",
    "    b_dec_scaled = int(b_dec * decimal_scale)\n",
    "    c_dec_scaled = int(c_dec * decimal_scale)\n",
    "    \n",
    "    # 再使用log1p归一化小数部分\n",
    "    max_dec_log = np.log1p(max_value * 2)\n",
    "    a_dec_norm = np.log1p(a_dec_scaled) / max_dec_log\n",
    "    b_dec_norm = np.log1p(b_dec_scaled) / max_dec_log\n",
    "    c_dec_norm = np.log1p(c_dec_scaled) / max_dec_log\n",
    "    \n",
    "    return [a_int_norm, a_dec_norm, b_int_norm, b_dec_norm, c_int_norm, c_dec_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mixed_data(num_samples=80000):\n",
    "    \"\"\"生成混合难度的数据集，包含不同范围的加法问题和困难案例\"\"\"\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    # 每个难度范围的样本数量\n",
    "    samples_per_range = num_samples // 4  # 平均分配到4个难度级别\n",
    "    \n",
    "    # 生成四个不同难度的数据\n",
    "    for max_value in [10, 100, 1000, 10000]:\n",
    "        # 普通样本数量\n",
    "        base_samples = int(samples_per_range * 0.7)\n",
    "        hard_samples = samples_per_range - base_samples\n",
    "        \n",
    "        # 普通样本\n",
    "        for _ in range(base_samples):\n",
    "            a = np.random.uniform(0, max_value)\n",
    "            b = np.random.uniform(0, max_value)\n",
    "            \n",
    "            if np.random.random() < 0.5:\n",
    "                c = a + b  # 正确答案\n",
    "                label = [0, 1]\n",
    "            else:\n",
    "                error_rate = np.random.uniform(0.01, 0.1)\n",
    "                error = (a + b) * error_rate * (1 if np.random.random() < 0.5 else -1)\n",
    "                c = a + b + error\n",
    "                label = [1, 0]\n",
    "            \n",
    "            features = normalize_data_log(a, b, c, 10000)\n",
    "            data.append(features)\n",
    "            labels.append(label)\n",
    "        \n",
    "        # 困难样本：一大一小\n",
    "        n_hard1 = hard_samples // 3\n",
    "        for _ in range(n_hard1):\n",
    "            a = np.random.uniform(max_value*0.8, max_value)\n",
    "            b = np.random.uniform(0.0001, 0.01)\n",
    "            \n",
    "            if np.random.random() < 0.5:\n",
    "                c = a + b\n",
    "                label = [0, 1]\n",
    "            else:\n",
    "                error = np.random.uniform(0.0001, 0.001) * (1 if np.random.random() < 0.5 else -1)\n",
    "                c = a + b + error\n",
    "                label = [1, 0]\n",
    "            \n",
    "            features = normalize_data_log(a, b, c, 10000)\n",
    "            data.append(features)\n",
    "            labels.append(label)\n",
    "        \n",
    "        # 困难样本：小数位精度测试\n",
    "        n_hard2 = hard_samples - n_hard1\n",
    "        for _ in range(n_hard2):\n",
    "            a = np.random.uniform(0, max_value)\n",
    "            b = np.random.uniform(0, max_value)\n",
    "            correct_sum = a + b\n",
    "            \n",
    "            if np.random.random() < 0.5:\n",
    "                c = correct_sum\n",
    "                label = [0, 1]\n",
    "            else:\n",
    "                # 非常接近但不等于正确答案\n",
    "                tiny_error = np.random.uniform(0.00001, 0.0001) * (1 if np.random.random() < 0.5 else -1)\n",
    "                c = correct_sum + tiny_error\n",
    "                label = [1, 0]\n",
    "                \n",
    "            features = normalize_data_log(a, b, c, 10000)\n",
    "            data.append(features)\n",
    "            labels.append(label)\n",
    "    \n",
    "    # 打乱数据\n",
    "    combined = list(zip(data, labels))\n",
    "    np.random.shuffle(combined)\n",
    "    data, labels = zip(*combined)\n",
    "    \n",
    "    return torch.tensor(data, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_addition(model, a, b, c, max_value=10000):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 获取适当的精度\n",
    "        if max_value <= 10:\n",
    "            decimal_places = 1\n",
    "        elif max_value <= 100:\n",
    "            decimal_places = 2\n",
    "        elif max_value <= 1000:\n",
    "            decimal_places = 3\n",
    "        else:\n",
    "            decimal_places = 4\n",
    "            \n",
    "        # 将a, b, c截断到指定小数位\n",
    "        factor = 10**decimal_places\n",
    "        a_truncated = math.floor(a * factor) / factor\n",
    "        b_truncated = math.floor(b * factor) / factor\n",
    "        c_truncated = math.floor(c * factor) / factor\n",
    "        \n",
    "        # 验证加法是否正确，只考虑到指定小数位\n",
    "        correct = abs((a_truncated + b_truncated) - c_truncated) < (0.5 * 10**(-decimal_places))\n",
    "        \n",
    "        # 使用模型预测\n",
    "        features = normalize_data_log(a, b, c, max_value)\n",
    "        input_data = torch.tensor([features], dtype=torch.float32).to(device)\n",
    "        probabilities = model(input_data)\n",
    "        \n",
    "        return probabilities[0, 1].item(), correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curriculum_training(model, loss_fn, optimizer, epochs_per_stage=150, final_stage_epochs=300, stage_losss_break=0.5, final_accuracy_break=0.95):\n",
    "    stages = [\n",
    "        {\"max_value\": 10, \"name\": \"10以内加法\", \"decimal_precision\": 1},\n",
    "        {\"max_value\": 100, \"name\": \"100以内加法\", \"decimal_precision\": 2},\n",
    "        {\"max_value\": 1000, \"name\": \"1000以内加法\", \"decimal_precision\": 3},\n",
    "        {\"max_value\": 10000, \"name\": \"10000以内加法\", \"decimal_precision\": 4}\n",
    "    ]\n",
    "    \n",
    "    # 为每个阶段保存一个检查点\n",
    "    for stage_idx, stage in enumerate(stages):\n",
    "        print(f\"\\n开始训练阶段 {stage_idx+1}: {stage['name']}\")\n",
    "        max_value = stage[\"max_value\"]\n",
    "        decimal_precision = stage[\"decimal_precision\"]\n",
    "        \n",
    "        # 为当前难度生成数据\n",
    "        X_train, y_train = generate_data(num_samples=8000, max_value=max_value)\n",
    "        X_val, y_val = generate_data(num_samples=1000, max_value=max_value)\n",
    "        \n",
    "        # 将数据移到设备\n",
    "        X_train = X_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        X_val = X_val.to(device)\n",
    "        y_val = y_val.to(device)\n",
    "        \n",
    "        # 训练当前阶段\n",
    "        batch_size = 128\n",
    "        for epoch in range(epochs_per_stage):\n",
    "            # 训练模式\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            indices = torch.randperm(len(X_train))\n",
    "            \n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                batch_indices = indices[i:i+batch_size]\n",
    "                inputs = X_train[batch_indices]\n",
    "                targets = y_train[batch_indices]\n",
    "                \n",
    "                # 前向传播\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                \n",
    "                # 反向传播和优化\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            train_loss = total_loss / (len(X_train) / batch_size)\n",
    "            \n",
    "            # 验证\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(X_val)\n",
    "                val_loss = loss_fn(val_outputs, y_val).item()\n",
    "                \n",
    "                # 计算准确率 - 使用当前阶段的小数精度要求\n",
    "                val_accuracy = evaluate_with_precision(model, X_val, y_val, decimal_precision)\n",
    "            \n",
    "            print(f'阶段 {stage_idx+1} 轮次 {epoch+1}/{epochs_per_stage}, '\n",
    "                  f'训练损失: {train_loss:.4f}, 验证损失: {val_loss:.4f}, '\n",
    "                  f'验证准确率: {val_accuracy:.4f}')\n",
    "            \n",
    "            # 如果验证准确率达到高水平，提前进入下一阶段\n",
    "            if val_loss < stage_losss_break:\n",
    "                print(f\"阶段 {stage_idx+1} 提前完成! 验证损失: {val_loss:.4f}\")\n",
    "                break\n",
    "                \n",
    "        # 保存阶段检查点\n",
    "        torch.save(model.state_dict(), f\"model_stage_{stage_idx+1}.pt\")\n",
    "        \n",
    "    # 最终阶段：混合数据训练\n",
    "    print(\"\\n开始最终整合阶段: 混合数据训练\")\n",
    "    X_train, y_train = generate_mixed_data()  # 生成各个难度级别的混合数据\n",
    "    X_val, y_val = generate_mixed_data(num_samples=2000)\n",
    "    \n",
    "    # 将数据移到设备\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_val = X_val.to(device)\n",
    "    y_val = y_val.to(device)\n",
    "    batch_size = 1024\n",
    "    \n",
    "    # 最终训练\n",
    "    for epoch in range(final_stage_epochs):\n",
    "        # 训练模式\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        indices = torch.randperm(len(X_train))\n",
    "        \n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            inputs = X_train[batch_indices]\n",
    "            targets = y_train[batch_indices]\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        train_loss = total_loss / (len(X_train) / batch_size)\n",
    "        \n",
    "        # 验证 - 使用最严格的精度要求\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val)\n",
    "            val_loss = loss_fn(val_outputs, y_val).item()\n",
    "            \n",
    "            # 使用最高精度要求评估\n",
    "            val_accuracy = evaluate_with_precision(model, X_val, y_val, decimal_precision=4)\n",
    "        \n",
    "        print(f'最终阶段 轮次 {epoch+1}/{final_stage_epochs}, '\n",
    "              f'训练损失: {train_loss:.4f}, 验证损失: {val_loss:.4f}, '\n",
    "              f'验证准确率: {val_accuracy:.4f}')\n",
    "        if val_accuracy > final_accuracy_break:\n",
    "            print(f\"最终阶段 提前完成! 验证准确率: {val_accuracy:.4f}\")\n",
    "            break\n",
    "    \n",
    "    # 保存最终模型\n",
    "    torch.save(model.state_dict(), \"model_final.pt\")\n",
    "    return model\n",
    "\n",
    "def evaluate_with_precision(model, X, y, decimal_precision):\n",
    "    \"\"\"根据指定的小数精度评估模型准确率\"\"\"\n",
    "    outputs = model(X)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    _, true_labels = torch.max(y, 1)\n",
    "    \n",
    "    # 使用原始数据重建a,b,c\n",
    "    # 注意：这里需要修改，因为数据格式改变了，需要从特征中提取回a,b,c\n",
    "    # 假设这里有一个函数可以从特征中恢复原始值\n",
    "    \n",
    "    correct_count = 0\n",
    "    total_count = len(X)\n",
    "    \n",
    "    for i in range(total_count):\n",
    "        # 获取预测标签和真实标签\n",
    "        pred_label = predicted[i].item()\n",
    "        true_label = true_labels[i].item()\n",
    "        \n",
    "        # 预测标签和真实标签一致，直接计数为正确\n",
    "        if pred_label == true_label:\n",
    "            correct_count += 1\n",
    "        \n",
    "    return correct_count / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "开始训练阶段 1: 10以内加法\n",
      "阶段 1 轮次 1/150, 训练损失: 0.6979, 验证损失: 0.6923, 验证准确率: 0.5140\n",
      "阶段 1 轮次 2/150, 训练损失: 0.6928, 验证损失: 0.6860, 验证准确率: 0.5360\n",
      "阶段 1 轮次 3/150, 训练损失: 0.6851, 验证损失: 0.6769, 验证准确率: 0.5800\n",
      "阶段 1 轮次 4/150, 训练损失: 0.6620, 验证损失: 0.6772, 验证准确率: 0.5680\n",
      "阶段 1 轮次 5/150, 训练损失: 0.5938, 验证损失: 0.5837, 验证准确率: 0.6610\n",
      "阶段 1 轮次 6/150, 训练损失: 0.5705, 验证损失: 0.5527, 验证准确率: 0.6830\n",
      "阶段 1 轮次 7/150, 训练损失: 0.5416, 验证损失: 0.5540, 验证准确率: 0.6900\n",
      "阶段 1 轮次 8/150, 训练损失: 0.5260, 验证损失: 0.5343, 验证准确率: 0.6870\n",
      "阶段 1 轮次 9/150, 训练损失: 0.5078, 验证损失: 0.5166, 验证准确率: 0.6950\n",
      "阶段 1 轮次 10/150, 训练损失: 0.5089, 验证损失: 0.5221, 验证准确率: 0.6970\n",
      "阶段 1 轮次 11/150, 训练损失: 0.5128, 验证损失: 0.5187, 验证准确率: 0.7040\n",
      "阶段 1 轮次 12/150, 训练损失: 0.4891, 验证损失: 0.5133, 验证准确率: 0.7060\n",
      "阶段 1 轮次 13/150, 训练损失: 0.5037, 验证损失: 0.5259, 验证准确率: 0.6880\n",
      "阶段 1 轮次 14/150, 训练损失: 0.4933, 验证损失: 0.5073, 验证准确率: 0.7280\n",
      "阶段 1 轮次 15/150, 训练损失: 0.4658, 验证损失: 0.4917, 验证准确率: 0.7250\n",
      "阶段 1 轮次 16/150, 训练损失: 0.4502, 验证损失: 0.4889, 验证准确率: 0.7280\n",
      "阶段 1 轮次 17/150, 训练损失: 0.4482, 验证损失: 0.5227, 验证准确率: 0.7110\n",
      "阶段 1 轮次 18/150, 训练损失: 0.4666, 验证损失: 0.5141, 验证准确率: 0.7230\n",
      "阶段 1 轮次 19/150, 训练损失: 0.4737, 验证损失: 0.4851, 验证准确率: 0.7370\n",
      "阶段 1 轮次 20/150, 训练损失: 0.4353, 验证损失: 0.4822, 验证准确率: 0.7360\n",
      "阶段 1 轮次 21/150, 训练损失: 0.4542, 验证损失: 0.5044, 验证准确率: 0.7150\n",
      "阶段 1 轮次 22/150, 训练损失: 0.4414, 验证损失: 0.4827, 验证准确率: 0.7300\n",
      "阶段 1 轮次 23/150, 训练损失: 0.4494, 验证损失: 0.4741, 验证准确率: 0.7330\n",
      "阶段 1 轮次 24/150, 训练损失: 0.4367, 验证损失: 0.4874, 验证准确率: 0.7480\n",
      "阶段 1 轮次 25/150, 训练损失: 0.4734, 验证损失: 0.5200, 验证准确率: 0.6870\n",
      "阶段 1 轮次 26/150, 训练损失: 0.4735, 验证损失: 0.4768, 验证准确率: 0.7320\n",
      "阶段 1 轮次 27/150, 训练损失: 0.4317, 验证损失: 0.5119, 验证准确率: 0.7060\n",
      "阶段 1 轮次 28/150, 训练损失: 0.4393, 验证损失: 0.4696, 验证准确率: 0.7520\n",
      "阶段 1 轮次 29/150, 训练损失: 0.4393, 验证损失: 0.4859, 验证准确率: 0.7450\n",
      "阶段 1 轮次 30/150, 训练损失: 0.4251, 验证损失: 0.4668, 验证准确率: 0.7390\n",
      "阶段 1 轮次 31/150, 训练损失: 0.4178, 验证损失: 0.4830, 验证准确率: 0.7330\n",
      "阶段 1 轮次 32/150, 训练损失: 0.4360, 验证损失: 0.4812, 验证准确率: 0.7320\n",
      "阶段 1 轮次 33/150, 训练损失: 0.4202, 验证损失: 0.4677, 验证准确率: 0.7560\n",
      "阶段 1 轮次 34/150, 训练损失: 0.4820, 验证损失: 0.5053, 验证准确率: 0.7250\n",
      "阶段 1 轮次 35/150, 训练损失: 0.4417, 验证损失: 0.4641, 验证准确率: 0.7620\n",
      "阶段 1 轮次 36/150, 训练损失: 0.4192, 验证损失: 0.5100, 验证准确率: 0.7220\n",
      "阶段 1 轮次 37/150, 训练损失: 0.4272, 验证损失: 0.5002, 验证准确率: 0.7120\n",
      "阶段 1 轮次 38/150, 训练损失: 0.4594, 验证损失: 0.4852, 验证准确率: 0.7290\n",
      "阶段 1 轮次 39/150, 训练损失: 0.4333, 验证损失: 0.4550, 验证准确率: 0.7580\n",
      "阶段 1 轮次 40/150, 训练损失: 0.4061, 验证损失: 0.4819, 验证准确率: 0.7410\n",
      "阶段 1 轮次 41/150, 训练损失: 0.4123, 验证损失: 0.4861, 验证准确率: 0.7520\n",
      "阶段 1 轮次 42/150, 训练损失: 0.4113, 验证损失: 0.4748, 验证准确率: 0.7600\n",
      "阶段 1 轮次 43/150, 训练损失: 0.4112, 验证损失: 0.4875, 验证准确率: 0.7320\n",
      "阶段 1 轮次 44/150, 训练损失: 0.4483, 验证损失: 0.4736, 验证准确率: 0.7460\n",
      "阶段 1 轮次 45/150, 训练损失: 0.4040, 验证损失: 0.4647, 验证准确率: 0.7580\n",
      "阶段 1 轮次 46/150, 训练损失: 0.4068, 验证损失: 0.5192, 验证准确率: 0.7480\n",
      "阶段 1 轮次 47/150, 训练损失: 0.4185, 验证损失: 0.5349, 验证准确率: 0.7150\n",
      "阶段 1 轮次 48/150, 训练损失: 0.4250, 验证损失: 0.4660, 验证准确率: 0.7570\n",
      "阶段 1 轮次 49/150, 训练损失: 0.4164, 验证损失: 0.4841, 验证准确率: 0.7370\n",
      "阶段 1 轮次 50/150, 训练损失: 0.4142, 验证损失: 0.4919, 验证准确率: 0.7240\n",
      "阶段 1 轮次 51/150, 训练损失: 0.4232, 验证损失: 0.4625, 验证准确率: 0.7540\n",
      "阶段 1 轮次 52/150, 训练损失: 0.4034, 验证损失: 0.4625, 验证准确率: 0.7500\n",
      "阶段 1 轮次 53/150, 训练损失: 0.4337, 验证损失: 0.4884, 验证准确率: 0.7410\n",
      "阶段 1 轮次 54/150, 训练损失: 0.4020, 验证损失: 0.4779, 验证准确率: 0.7460\n",
      "阶段 1 轮次 55/150, 训练损失: 0.4223, 验证损失: 0.4697, 验证准确率: 0.7500\n",
      "阶段 1 轮次 56/150, 训练损失: 0.4014, 验证损失: 0.4802, 验证准确率: 0.7440\n",
      "阶段 1 轮次 57/150, 训练损失: 0.4013, 验证损失: 0.4863, 验证准确率: 0.7490\n",
      "阶段 1 轮次 58/150, 训练损失: 0.4531, 验证损失: 0.4700, 验证准确率: 0.7450\n",
      "阶段 1 轮次 59/150, 训练损失: 0.4014, 验证损失: 0.4690, 验证准确率: 0.7590\n",
      "阶段 1 轮次 60/150, 训练损失: 0.4034, 验证损失: 0.4737, 验证准确率: 0.7530\n",
      "阶段 1 轮次 61/150, 训练损失: 0.4235, 验证损失: 0.5148, 验证准确率: 0.7270\n",
      "阶段 1 轮次 62/150, 训练损失: 0.4063, 验证损失: 0.4845, 验证准确率: 0.7530\n",
      "阶段 1 轮次 63/150, 训练损失: 0.3951, 验证损失: 0.4790, 验证准确率: 0.7490\n",
      "阶段 1 轮次 64/150, 训练损失: 0.3982, 验证损失: 0.4767, 验证准确率: 0.7510\n",
      "阶段 1 轮次 65/150, 训练损失: 0.3921, 验证损失: 0.5301, 验证准确率: 0.7250\n",
      "阶段 1 轮次 66/150, 训练损失: 0.3970, 验证损失: 0.5784, 验证准确率: 0.7020\n",
      "阶段 1 轮次 67/150, 训练损失: 0.4045, 验证损失: 0.4901, 验证准确率: 0.7440\n",
      "阶段 1 轮次 68/150, 训练损失: 0.3910, 验证损失: 0.5121, 验证准确率: 0.7310\n",
      "阶段 1 轮次 69/150, 训练损失: 0.3991, 验证损失: 0.4942, 验证准确率: 0.7570\n",
      "阶段 1 轮次 70/150, 训练损失: 0.3889, 验证损失: 0.4882, 验证准确率: 0.7580\n",
      "阶段 1 轮次 71/150, 训练损失: 0.3896, 验证损失: 0.4816, 验证准确率: 0.7530\n",
      "阶段 1 轮次 72/150, 训练损失: 0.4083, 验证损失: 0.4922, 验证准确率: 0.7420\n",
      "阶段 1 轮次 73/150, 训练损失: 0.4052, 验证损失: 0.5084, 验证准确率: 0.7550\n",
      "阶段 1 轮次 74/150, 训练损失: 0.3962, 验证损失: 0.4982, 验证准确率: 0.7460\n",
      "阶段 1 轮次 75/150, 训练损失: 0.3952, 验证损失: 0.5028, 验证准确率: 0.7420\n",
      "阶段 1 轮次 76/150, 训练损失: 0.4034, 验证损失: 0.5274, 验证准确率: 0.7420\n",
      "阶段 1 轮次 77/150, 训练损失: 0.4419, 验证损失: 0.4900, 验证准确率: 0.7480\n",
      "阶段 1 轮次 78/150, 训练损失: 0.3966, 验证损失: 0.4916, 验证准确率: 0.7440\n",
      "阶段 1 轮次 79/150, 训练损失: 0.3853, 验证损失: 0.4908, 验证准确率: 0.7420\n",
      "阶段 1 轮次 80/150, 训练损失: 0.3943, 验证损失: 0.5114, 验证准确率: 0.7540\n",
      "阶段 1 轮次 81/150, 训练损失: 0.3815, 验证损失: 0.5051, 验证准确率: 0.7440\n",
      "阶段 1 轮次 82/150, 训练损失: 0.3903, 验证损失: 0.5241, 验证准确率: 0.7340\n",
      "阶段 1 轮次 83/150, 训练损失: 0.3905, 验证损失: 0.4982, 验证准确率: 0.7540\n",
      "阶段 1 轮次 84/150, 训练损失: 0.4340, 验证损失: 0.4991, 验证准确率: 0.7300\n",
      "阶段 1 轮次 85/150, 训练损失: 0.3977, 验证损失: 0.5017, 验证准确率: 0.7410\n",
      "阶段 1 轮次 86/150, 训练损失: 0.3840, 验证损失: 0.4839, 验证准确率: 0.7450\n",
      "阶段 1 轮次 87/150, 训练损失: 0.3790, 验证损失: 0.5024, 验证准确率: 0.7400\n",
      "阶段 1 轮次 88/150, 训练损失: 0.3808, 验证损失: 0.5569, 验证准确率: 0.7250\n",
      "阶段 1 轮次 89/150, 训练损失: 0.3870, 验证损失: 0.5538, 验证准确率: 0.7190\n",
      "阶段 1 轮次 90/150, 训练损失: 0.4022, 验证损失: 0.5029, 验证准确率: 0.7430\n",
      "阶段 1 轮次 91/150, 训练损失: 0.3803, 验证损失: 0.5005, 验证准确率: 0.7480\n",
      "阶段 1 轮次 92/150, 训练损失: 0.3829, 验证损失: 0.5148, 验证准确率: 0.7240\n",
      "阶段 1 轮次 93/150, 训练损失: 0.3925, 验证损失: 0.5311, 验证准确率: 0.7390\n",
      "阶段 1 轮次 94/150, 训练损失: 0.3794, 验证损失: 0.5307, 验证准确率: 0.7490\n",
      "阶段 1 轮次 95/150, 训练损失: 0.3770, 验证损失: 0.5171, 验证准确率: 0.7420\n",
      "阶段 1 轮次 96/150, 训练损失: 0.3769, 验证损失: 0.5116, 验证准确率: 0.7420\n",
      "阶段 1 轮次 97/150, 训练损失: 0.4003, 验证损失: 0.5320, 验证准确率: 0.7110\n",
      "阶段 1 轮次 98/150, 训练损失: 0.4019, 验证损失: 0.5001, 验证准确率: 0.7340\n",
      "阶段 1 轮次 99/150, 训练损失: 0.3889, 验证损失: 0.5154, 验证准确率: 0.7470\n",
      "阶段 1 轮次 100/150, 训练损失: 0.3750, 验证损失: 0.5354, 验证准确率: 0.7370\n",
      "阶段 1 轮次 101/150, 训练损失: 0.3945, 验证损失: 0.5473, 验证准确率: 0.7170\n",
      "阶段 1 轮次 102/150, 训练损失: 0.3855, 验证损失: 0.5206, 验证准确率: 0.7440\n",
      "阶段 1 轮次 103/150, 训练损失: 0.3841, 验证损失: 0.5130, 验证准确率: 0.7440\n",
      "阶段 1 轮次 104/150, 训练损失: 0.3805, 验证损失: 0.5220, 验证准确率: 0.7370\n",
      "阶段 1 轮次 105/150, 训练损失: 0.3761, 验证损失: 0.5187, 验证准确率: 0.7390\n",
      "阶段 1 轮次 106/150, 训练损失: 0.3814, 验证损失: 0.5115, 验证准确率: 0.7260\n",
      "阶段 1 轮次 107/150, 训练损失: 0.3820, 验证损失: 0.5104, 验证准确率: 0.7470\n",
      "阶段 1 轮次 108/150, 训练损失: 0.3693, 验证损失: 0.5109, 验证准确率: 0.7410\n",
      "阶段 1 轮次 109/150, 训练损失: 0.3654, 验证损失: 0.5324, 验证准确率: 0.7400\n",
      "阶段 1 轮次 110/150, 训练损失: 0.3655, 验证损失: 0.5258, 验证准确率: 0.7360\n",
      "阶段 1 轮次 111/150, 训练损失: 0.3638, 验证损失: 0.5554, 验证准确率: 0.7160\n",
      "阶段 1 轮次 112/150, 训练损失: 0.3723, 验证损失: 0.5010, 验证准确率: 0.7400\n",
      "阶段 1 轮次 113/150, 训练损失: 0.3715, 验证损失: 0.5206, 验证准确率: 0.7490\n",
      "阶段 1 轮次 114/150, 训练损失: 0.3642, 验证损失: 0.5209, 验证准确率: 0.7370\n",
      "阶段 1 轮次 115/150, 训练损失: 0.3657, 验证损失: 0.5187, 验证准确率: 0.7390\n",
      "阶段 1 轮次 116/150, 训练损失: 0.3638, 验证损失: 0.5300, 验证准确率: 0.7380\n",
      "阶段 1 轮次 117/150, 训练损失: 0.3605, 验证损失: 0.5410, 验证准确率: 0.7270\n",
      "阶段 1 轮次 118/150, 训练损失: 0.4310, 验证损失: 0.5058, 验证准确率: 0.7440\n",
      "阶段 1 轮次 119/150, 训练损失: 0.3801, 验证损失: 0.5429, 验证准确率: 0.7210\n",
      "阶段 1 轮次 120/150, 训练损失: 0.3673, 验证损失: 0.5247, 验证准确率: 0.7380\n",
      "阶段 1 轮次 121/150, 训练损失: 0.3609, 验证损失: 0.5675, 验证准确率: 0.7220\n",
      "阶段 1 轮次 122/150, 训练损失: 0.3618, 验证损失: 0.5582, 验证准确率: 0.7470\n",
      "阶段 1 轮次 123/150, 训练损失: 0.3756, 验证损失: 0.5479, 验证准确率: 0.7320\n",
      "阶段 1 轮次 124/150, 训练损失: 0.3641, 验证损失: 0.5572, 验证准确率: 0.7430\n",
      "阶段 1 轮次 125/150, 训练损失: 0.3717, 验证损失: 0.5551, 验证准确率: 0.7300\n",
      "阶段 1 轮次 126/150, 训练损失: 0.3555, 验证损失: 0.5505, 验证准确率: 0.7530\n",
      "阶段 1 轮次 127/150, 训练损失: 0.3559, 验证损失: 0.5442, 验证准确率: 0.7280\n",
      "阶段 1 轮次 128/150, 训练损失: 0.3559, 验证损失: 0.5695, 验证准确率: 0.7290\n",
      "阶段 1 轮次 129/150, 训练损失: 0.3588, 验证损失: 0.7063, 验证准确率: 0.7090\n",
      "阶段 1 轮次 130/150, 训练损失: 0.3610, 验证损失: 0.6428, 验证准确率: 0.7190\n",
      "阶段 1 轮次 131/150, 训练损失: 0.3599, 验证损失: 0.5855, 验证准确率: 0.7320\n",
      "阶段 1 轮次 132/150, 训练损失: 0.3600, 验证损失: 0.5884, 验证准确率: 0.7310\n",
      "阶段 1 轮次 133/150, 训练损失: 0.3728, 验证损失: 0.6141, 验证准确率: 0.7290\n",
      "阶段 1 轮次 134/150, 训练损失: 0.3711, 验证损失: 0.5971, 验证准确率: 0.7190\n",
      "阶段 1 轮次 135/150, 训练损失: 0.3625, 验证损失: 0.6340, 验证准确率: 0.7460\n",
      "阶段 1 轮次 136/150, 训练损失: 0.3446, 验证损失: 0.5830, 验证准确率: 0.7280\n",
      "阶段 1 轮次 137/150, 训练损失: 0.3905, 验证损失: 0.5533, 验证准确率: 0.7220\n",
      "阶段 1 轮次 138/150, 训练损失: 0.3514, 验证损失: 0.5773, 验证准确率: 0.7380\n",
      "阶段 1 轮次 139/150, 训练损失: 0.3702, 验证损失: 0.6752, 验证准确率: 0.7140\n",
      "阶段 1 轮次 140/150, 训练损失: 0.3697, 验证损失: 0.6010, 验证准确率: 0.7260\n",
      "阶段 1 轮次 141/150, 训练损失: 0.3491, 验证损失: 0.5462, 验证准确率: 0.7270\n",
      "阶段 1 轮次 142/150, 训练损失: 0.3440, 验证损失: 0.5740, 验证准确率: 0.7330\n",
      "阶段 1 轮次 143/150, 训练损失: 0.3490, 验证损失: 0.5922, 验证准确率: 0.7440\n",
      "阶段 1 轮次 144/150, 训练损失: 0.3516, 验证损失: 0.6697, 验证准确率: 0.7420\n",
      "阶段 1 轮次 145/150, 训练损失: 0.3489, 验证损失: 0.6043, 验证准确率: 0.7360\n",
      "阶段 1 轮次 146/150, 训练损失: 0.3549, 验证损失: 0.5941, 验证准确率: 0.7100\n",
      "阶段 1 轮次 147/150, 训练损失: 0.3469, 验证损失: 0.6830, 验证准确率: 0.7100\n",
      "阶段 1 轮次 148/150, 训练损失: 0.3510, 验证损失: 0.5894, 验证准确率: 0.7150\n",
      "阶段 1 轮次 149/150, 训练损失: 0.3628, 验证损失: 0.6578, 验证准确率: 0.7330\n",
      "阶段 1 轮次 150/150, 训练损失: 0.3659, 验证损失: 0.6421, 验证准确率: 0.7320\n",
      "\n",
      "开始训练阶段 2: 100以内加法\n",
      "阶段 2 轮次 1/150, 训练损失: 0.9099, 验证损失: 0.6367, 验证准确率: 0.6440\n",
      "阶段 2 轮次 2/150, 训练损失: 0.5845, 验证损失: 0.5546, 验证准确率: 0.7330\n",
      "阶段 2 轮次 3/150, 训练损失: 0.5225, 验证损失: 0.4523, 验证准确率: 0.7640\n",
      "阶段 2 轮次 4/150, 训练损失: 0.4664, 验证损失: 0.4864, 验证准确率: 0.7650\n",
      "阶段 2 轮次 5/150, 训练损失: 0.4871, 验证损失: 0.4508, 验证准确率: 0.7870\n",
      "阶段 2 轮次 6/150, 训练损失: 0.4728, 验证损失: 0.4439, 验证准确率: 0.7770\n",
      "阶段 2 轮次 7/150, 训练损失: 0.4458, 验证损失: 0.4549, 验证准确率: 0.7650\n",
      "阶段 2 轮次 8/150, 训练损失: 0.4395, 验证损失: 0.4205, 验证准确率: 0.8000\n",
      "阶段 2 轮次 9/150, 训练损失: 0.4405, 验证损失: 0.4357, 验证准确率: 0.8020\n",
      "阶段 2 轮次 10/150, 训练损失: 0.4313, 验证损失: 0.4191, 验证准确率: 0.8110\n",
      "阶段 2 轮次 11/150, 训练损失: 0.4429, 验证损失: 0.4227, 验证准确率: 0.7950\n",
      "阶段 2 轮次 12/150, 训练损失: 0.4167, 验证损失: 0.4020, 验证准确率: 0.8060\n",
      "阶段 2 轮次 13/150, 训练损失: 0.4154, 验证损失: 0.3943, 验证准确率: 0.8120\n",
      "阶段 2 轮次 14/150, 训练损失: 0.4769, 验证损失: 0.4413, 验证准确率: 0.7940\n",
      "阶段 2 轮次 15/150, 训练损失: 0.4189, 验证损失: 0.3989, 验证准确率: 0.8110\n",
      "阶段 2 轮次 16/150, 训练损失: 0.3923, 验证损失: 0.4028, 验证准确率: 0.8070\n",
      "阶段 2 轮次 17/150, 训练损失: 0.3845, 验证损失: 0.3741, 验证准确率: 0.8290\n",
      "阶段 2 轮次 18/150, 训练损失: 0.4158, 验证损失: 0.3990, 验证准确率: 0.8090\n",
      "阶段 2 轮次 19/150, 训练损失: 0.3755, 验证损失: 0.3735, 验证准确率: 0.8230\n",
      "阶段 2 轮次 20/150, 训练损失: 0.3795, 验证损失: 0.3671, 验证准确率: 0.8140\n",
      "阶段 2 轮次 21/150, 训练损失: 0.3663, 验证损失: 0.4053, 验证准确率: 0.8010\n",
      "阶段 2 轮次 22/150, 训练损失: 0.3620, 验证损失: 0.3942, 验证准确率: 0.8120\n",
      "阶段 2 轮次 23/150, 训练损失: 0.3747, 验证损失: 0.4051, 验证准确率: 0.8070\n",
      "阶段 2 轮次 24/150, 训练损失: 0.3772, 验证损失: 0.4285, 验证准确率: 0.7970\n",
      "阶段 2 轮次 25/150, 训练损失: 0.3730, 验证损失: 0.4007, 验证准确率: 0.8180\n",
      "阶段 2 轮次 26/150, 训练损失: 0.3560, 验证损失: 0.3692, 验证准确率: 0.8490\n",
      "阶段 2 轮次 27/150, 训练损失: 0.3646, 验证损失: 0.3909, 验证准确率: 0.8380\n",
      "阶段 2 轮次 28/150, 训练损失: 0.3500, 验证损失: 0.3632, 验证准确率: 0.8570\n",
      "阶段 2 轮次 29/150, 训练损失: 0.3489, 验证损失: 0.3737, 验证准确率: 0.8430\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[117]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m criterion = nn.BCELoss()\n\u001b[32m     25\u001b[39m optimizer = optim.Adam(model.parameters(), lr=\u001b[32m0.001\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m model = \u001b[43mcurriculum_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs_per_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage_losss_break\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mfinal_accuracy_break\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.96\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# epochs = 300\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# batch_size = 1024\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     99\u001b[39m \n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# 测试示例\u001b[39;00m\n\u001b[32m    101\u001b[39m decimal_examples = [\n\u001b[32m    102\u001b[39m     \u001b[38;5;66;03m# 1-10：简单小数加法（正确结果）\u001b[39;00m\n\u001b[32m    103\u001b[39m     (\u001b[32m12.34\u001b[39m, \u001b[32m56.78\u001b[39m, \u001b[32m69.12\u001b[39m),       \u001b[38;5;66;03m# 正确\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    170\u001b[39m     (\u001b[32m0.1111\u001b[39m, \u001b[32m9999.8888\u001b[39m, \u001b[32m9999.9000\u001b[39m)\u001b[38;5;66;03m# 错误（正确应为9999.9999）\u001b[39;00m\n\u001b[32m    171\u001b[39m ]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mcurriculum_training\u001b[39m\u001b[34m(model, loss_fn, optimizer, epochs_per_stage, final_stage_epochs, stage_losss_break, final_accuracy_break)\u001b[39m\n\u001b[32m     36\u001b[39m targets = y_train[batch_indices]\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m loss = loss_fn(outputs, targets)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# 反向传播和优化\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learn_ai/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learn_ai/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mAdditionNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     14\u001b[39m x = torch.relu(\u001b[38;5;28mself\u001b[39m.fc3(x))\n\u001b[32m     15\u001b[39m x = torch.relu(\u001b[38;5;28mself\u001b[39m.fc4(x))\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m x = torch.softmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, dim=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# 使用softmax确保两个输出和为1\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learn_ai/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learn_ai/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learn_ai/venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# X_train, y_train = generate_data(num_samples=80000, max_value=10000)\n",
    "# X_val, y_val = generate_data(num_samples=1000, max_value=10000)\n",
    "# X_test, y_test = generate_data(num_samples=2000, max_value=10000)\n",
    "\n",
    "# # 将数据移到GPU\n",
    "# X_train = X_train.to(device)\n",
    "# y_train = y_train.to(device)\n",
    "# X_val = X_val.to(device)\n",
    "# y_val = y_val.to(device)\n",
    "# X_test = X_test.to(device)\n",
    "# y_test = y_test.to(device)\n",
    "\n",
    "def init_xavier(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        # 使用Xavier均匀分布初始化权重\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        # 将偏置初始化为小的正数，避免ReLU神经元\"死亡\"\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.01)\n",
    "\n",
    "# 创建模型并移到GPU\n",
    "model = AdditionNet().to(device)\n",
    "model.apply(init_xavier)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model = curriculum_training(model, criterion, optimizer, epochs_per_stage=150, stage_losss_break=0.3, \n",
    "                            final_accuracy_break=0.96)\n",
    "\n",
    "# 训练模型\n",
    "# epochs = 300\n",
    "# batch_size = 1024\n",
    "# losses = []\n",
    "# best_val_loss = float('inf')\n",
    "# patience = 100  # 早停耐心值\n",
    "# no_improve = 0\n",
    "# accuracy_threshold = 0.996\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "# \tmodel.train()\n",
    "# \ttotal_loss = 0\n",
    "# \t# 获取随机批次\n",
    "# \tindices = torch.randperm(len(X_train))\n",
    "# \tfor i in range(0, len(X_train), batch_size):\n",
    "# \t\tbatch_indices = indices[i:i+batch_size]\n",
    "# \t\tinputs = X_train[batch_indices]\n",
    "# \t\ttargets = y_train[batch_indices]\n",
    "\t\t\n",
    "# \t\t# 前向传播\n",
    "# \t\toutputs = model(inputs)\n",
    "# \t\tloss = criterion(outputs, targets)\n",
    "\t\t\n",
    "# \t\t# 反向传播和优化\n",
    "# \t\toptimizer.zero_grad()\n",
    "# \t\tloss.backward()\n",
    "# \t\toptimizer.step()\n",
    "\t\t\n",
    "# \t\ttotal_loss += loss.item()\n",
    "\t\t\n",
    "# \ttrain_loss = total_loss / (len(X_train) / batch_size)\n",
    "# \tmodel.eval()\n",
    "# \twith torch.no_grad():\n",
    "# \t\tval_outputs = model(X_val)\n",
    "# \t\tval_loss = criterion(val_outputs, y_val).item()\n",
    "\t\t\n",
    "# \t\t_, val_predicted = torch.max(val_outputs, 1)\n",
    "# \t\t_, val_true_labels = torch.max(y_val, 1)\n",
    "# \t\tval_accuracy = (val_predicted == val_true_labels).float().mean().item()\n",
    "\n",
    "# \tprint(f'轮次 {epoch+1}/{epochs}, 训练损失: {train_loss:.4f}, 验证损失: {val_loss:.4f}, 验证准确率: {val_accuracy:.4f}')\n",
    "\n",
    "# \t# 早停检查\n",
    "# \tif val_loss < best_val_loss:\n",
    "# \t\tbest_val_loss = val_loss\n",
    "# \t\ttorch.save(model.state_dict(), 'best_model.pt')\n",
    "# \t\tno_improve = 0\n",
    "# \telse:\n",
    "# \t\tno_improve += 1\n",
    "\n",
    "# \t# 准确率达到阈值时停止\n",
    "# \tif val_accuracy >= accuracy_threshold:\n",
    "# \t\tprint(f'验证准确率达到 {val_accuracy:.4f}，在第 {epoch+1} 轮停止训练')\n",
    "# \t\tbreak\n",
    "\t\t\n",
    "# \t# 早停检查\n",
    "# \tif no_improve >= patience:\n",
    "# \t\tprint(f'连续 {patience} 轮未改善，在第 {epoch+1} 轮停止训练')\n",
    "# \t\tbreak\n",
    "\n",
    "# # 修改check_addition函数以使用GPU\n",
    "# def check_addition(a, b, c, max_value=10000):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         a_norm, b_norm, c_norm = normalize_data_log(a, b, c, max_value)\n",
    "#         input_data = torch.tensor([[a_norm, b_norm, c_norm]], dtype=torch.float32).to(device)\n",
    "#         probabilities = model(input_data)\n",
    "#         # 打印原始输出，帮助调试\n",
    "#         print(f\"原始输出: {probabilities.cpu().numpy()}\")\n",
    "#         return probabilities[0, 1].item()  # 返回正确的概率(索引1)\n",
    "\n",
    "# 测试示例\n",
    "decimal_examples = [\n",
    "    # 1-10：简单小数加法（正确结果）\n",
    "    (12.34, 56.78, 69.12),       # 正确\n",
    "    (123.45, 678.90, 802.35),    # 正确\n",
    "    (0.75, 0.25, 1.00),          # 正确\n",
    "    (9.99, 1.01, 11.00),         # 正确\n",
    "    (45.67, 32.33, 78.00),       # 正确\n",
    "    \n",
    "    # 11-20：简单小数加法（错误结果）\n",
    "    (22.22, 33.33, 56.66),       # 错误（正确应为55.55）\n",
    "    (88.88, 11.11, 101.00),      # 错误（正确应为99.99）\n",
    "    (7.50, 2.50, 9.50),          # 错误（正确应为10.00）\n",
    "    (45.50, 45.50, 90.00),       # 错误（正确应为91.00）\n",
    "    (67.89, 12.10, 81.99),       # 错误（正确应为79.99）\n",
    "    \n",
    "    # 21-30：大数加法（正确结果）\n",
    "    (1234.56, 7890.12, 9124.68), # 正确\n",
    "    (8765.43, 1234.57, 10000.00),# 正确\n",
    "    (5432.10, 5432.10, 10864.20),# 正确\n",
    "    (9999.99, 0.01, 10000.00),   # 正确\n",
    "    (4567.89, 3210.11, 7778.00), # 正确\n",
    "    \n",
    "    # 31-40：大数加法（错误结果）\n",
    "    (2345.67, 8901.23, 11000.00),# 错误（正确应为11246.90）\n",
    "    (7777.77, 2222.22, 9999.00), # 错误（正确应为9999.99）\n",
    "    (4444.44, 5555.55, 9999.00), # 错误（正确应为9999.99）\n",
    "    (6789.01, 2345.67, 9035.68), # 错误（正确应为9134.68）\n",
    "    (5000.00, 5000.00, 9000.00), # 错误（正确应为10000.00）\n",
    "    \n",
    "    # 41-50：更复杂的小数加法（正确结果）\n",
    "    (123.456, 876.544, 1000.000),# 正确\n",
    "    (0.1234, 0.8766, 1.0000),    # 正确\n",
    "    (7.5432, 2.4568, 10.0000),   # 正确\n",
    "    (99.9999, 0.0001, 100.0000), # 正确\n",
    "    (567.123, 432.877, 1000.000),# 正确\n",
    "    \n",
    "    # 51-60：更复杂的小数加法（错误结果）\n",
    "    (456.789, 543.211, 999.999), # 错误（正确应为1000.000）\n",
    "    (1.2345, 2.3456, 3.5000),    # 错误（正确应为3.5801）\n",
    "    (6.7890, 3.2109, 9.9998),    # 错误（正确应为9.9999）\n",
    "    (0.0123, 0.0456, 0.0580),    # 错误（正确应为0.0579）\n",
    "    (7.1234, 8.9876, 16.0000),   # 错误（正确应为16.1110）\n",
    "    \n",
    "    # 61-70：接近相等的数加法（正确结果）\n",
    "    (999.999, 999.999, 1999.998),# 正确\n",
    "    (555.555, 555.555, 1111.110),# 正确\n",
    "    (123.123, 123.123, 246.246), # 正确\n",
    "    (7.7777, 7.7777, 15.5554),   # 正确\n",
    "    (0.5000, 0.5000, 1.0000),    # 正确\n",
    "    \n",
    "    # 71-80：接近相等的数加法（错误结果）\n",
    "    (444.444, 444.444, 888.000), # 错误（正确应为888.888）\n",
    "    (777.777, 777.777, 1555.555),# 错误（正确应为1555.554）\n",
    "    (3.3333, 3.3333, 6.6000),    # 错误（正确应为6.6666）\n",
    "    (1.1111, 1.1111, 2.2000),    # 错误（正确应为2.2222）\n",
    "    (9.9999, 9.9999, 19.0000),   # 错误（正确应为19.9998）\n",
    "    \n",
    "    # 81-90：一个数很大一个数很小加法（正确结果）\n",
    "    (9999.99, 0.01, 10000.00),   # 正确\n",
    "    (0.0001, 999.9999, 1000.0000),# 正确\n",
    "    (0.1234, 987.8766, 988.0000),# 正确\n",
    "    (1.0000, 9999.0000, 10000.0000),# 正确\n",
    "    (0.0001, 0.9999, 1.0000),    # 正确\n",
    "    \n",
    "    # 91-100：一个数很大一个数很小加法（错误结果）\n",
    "    (9876.54, 0.01, 9876.54),    # 错误（正确应为9876.55）\n",
    "    (0.0001, 100.0000, 100.0002),# 错误（正确应为100.0001）\n",
    "    (1234.56, 0.001, 1234.56),   # 错误（正确应为1234.561）\n",
    "    (0.0001, 9999.9998, 9999.0000),# 错误（正确应为9999.9999）\n",
    "    (0.1111, 9999.8888, 9999.9000)# 错误（正确应为9999.9999）\n",
    "]\n",
    "\n",
    "for a, b, c in decimal_examples:\n",
    "    prob = check_addition(model, a, b, c)\n",
    "    correct = abs((a + b) - c) < 1e-5\n",
    "    print(f'{a:.4f} + {b:.4f} = {c:.4f} 的正确概率: {prob:.4f}, 实际上是: {\"正确\" if correct else \"错误\"}, 检验结果是否正确: {\"正确\" if (correct and prob > 0.5) or (not correct and prob < 0.5) else \"错误\"}')\n",
    "# 统计检验结果与实际结果相符的次数与百分比\n",
    "total_examples = len(decimal_examples)\n",
    "correct_predictions = 0\n",
    "\n",
    "for a, b, c in decimal_examples:\n",
    "    prob = check_addition(model,a, b, c)\n",
    "    correct = abs((a + b) - c) < 1e-5\n",
    "    prediction_correct = (correct and prob > 0.5) or (not correct and prob < 0.5)\n",
    "    \n",
    "    if prediction_correct:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_examples * 100\n",
    "print(f\"\\n检验结果统计:\")\n",
    "print(f\"总样本数: {total_examples}\")\n",
    "print(f\"检验结果与实际结果相符的次数: {correct_predictions}\")\n",
    "print(f\"检验准确率: {accuracy:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始输出: [[0.30360958 0.69639045]]\n",
      "12.3400 + 56.7800 = 69.1200 的正确概率: 0.6964, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.06405322 0.93594676]]\n",
      "123.4500 + 678.9000 = 802.3500 的正确概率: 0.9359, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.32635722 0.67364275]]\n",
      "0.7500 + 0.2500 = 1.0000 的正确概率: 0.6736, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.07651028 0.92348975]]\n",
      "9.9900 + 1.0100 = 11.0000 的正确概率: 0.9235, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.16146506 0.83853495]]\n",
      "45.6700 + 32.3300 = 78.0000 的正确概率: 0.8385, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.90496624 0.09503371]]\n",
      "22.2200 + 33.3300 = 56.6600 的正确概率: 0.0950, 实际上是: 错误, 检验结果是否正确: 正确\n",
      "原始输出: [[0.1937114 0.8062886]]\n",
      "88.8800 + 11.1100 = 101.0000 的正确概率: 0.8063, 实际上是: 错误, 检验结果是否正确: 错误\n",
      "原始输出: [[0.940557   0.05944296]]\n",
      "7.5000 + 2.5000 = 9.5000 的正确概率: 0.0594, 实际上是: 错误, 检验结果是否正确: 正确\n",
      "原始输出: [[0.57528    0.42472002]]\n",
      "45.5000 + 45.5000 = 90.0000 的正确概率: 0.4247, 实际上是: 错误, 检验结果是否正确: 正确\n",
      "原始输出: [[0.80311286 0.19688709]]\n",
      "67.8900 + 12.1000 = 81.9900 的正确概率: 0.1969, 实际上是: 错误, 检验结果是否正确: 正确\n",
      "原始输出: [[0.07178336 0.9282167 ]]\n",
      "1234.5600 + 7890.1200 = 9124.6800 的正确概率: 0.9282, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.10883652 0.89116347]]\n",
      "8765.4300 + 1234.5700 = 10000.0000 的正确概率: 0.8912, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.2136742  0.78632575]]\n",
      "5432.1000 + 5432.1000 = 10864.2000 的正确概率: 0.7863, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.00243126 0.99756867]]\n",
      "9999.9900 + 0.0100 = 10000.0000 的正确概率: 0.9976, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.07932483 0.9206752 ]]\n",
      "4567.8900 + 3210.1100 = 7778.0000 的正确概率: 0.9207, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.8206536  0.17934638]]\n",
      "2345.6700 + 8901.2300 = 11000.0000 的正确概率: 0.1793, 实际上是: 错误, 检验结果是否正确: 正确\n",
      "原始输出: [[0.24290387 0.7570961 ]]\n",
      "7777.7700 + 2222.2200 = 9999.0000 的正确概率: 0.7571, 实际上是: 错误, 检验结果是否正确: 错误\n",
      "原始输出: [[0.11504443 0.8849556 ]]\n",
      "4444.4400 + 5555.5500 = 9999.0000 的正确概率: 0.8850, 实际上是: 错误, 检验结果是否正确: 错误\n",
      "原始输出: [[0.45011565 0.5498844 ]]\n",
      "6789.0100 + 2345.6700 = 9035.6800 的正确概率: 0.5499, 实际上是: 错误, 检验结果是否正确: 错误\n",
      "原始输出: [[9.9999988e-01 1.5307424e-07]]\n",
      "5000.0000 + 5000.0000 = 9000.0000 的正确概率: 0.0000, 实际上是: 错误, 检验结果是否正确: 正确\n",
      "原始输出: [[0.16216205 0.837838  ]]\n",
      "123.4560 + 876.5440 = 1000.0000 的正确概率: 0.8378, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.7369649  0.26303515]]\n",
      "0.1234 + 0.8766 = 1.0000 的正确概率: 0.2630, 实际上是: 正确, 检验结果是否正确: 错误\n",
      "原始输出: [[0.11988896 0.8801111 ]]\n",
      "7.5432 + 2.4568 = 10.0000 的正确概率: 0.8801, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.08979823 0.9102018 ]]\n",
      "99.9999 + 0.0001 = 100.0000 的正确概率: 0.9102, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.09880769 0.9011923 ]]\n",
      "567.1230 + 432.8770 = 1000.0000 的正确概率: 0.9012, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.07667679 0.92332315]]\n",
      "456.7890 + 543.2110 = 999.9990 的正确概率: 0.9233, 实际上是: 错误, 检验结果是否正确: 错误\n",
      "原始输出: [[0.5348556  0.46514437]]\n",
      "1.2345 + 2.3456 = 3.5000 的正确概率: 0.4651, 实际上是: 错误, 检验结果是否正确: 正确\n",
      "原始输出: [[0.13565497 0.8643451 ]]\n",
      "6.7890 + 3.2109 = 9.9998 的正确概率: 0.8643, 实际上是: 错误, 检验结果是否正确: 错误\n",
      "原始输出: [[0.9980895  0.00191051]]\n",
      "0.0123 + 0.0456 = 0.0580 的正确概率: 0.0019, 实际上是: 错误, 检验结果是否正确: 正确\n",
      "原始输出: [[0.18547806 0.8145219 ]]\n",
      "7.1234 + 8.9876 = 16.0000 的正确概率: 0.8145, 实际上是: 错误, 检验结果是否正确: 错误\n",
      "原始输出: [[0.1290495 0.8709505]]\n",
      "999.9990 + 999.9990 = 1999.9980 的正确概率: 0.8710, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.11123041 0.88876957]]\n",
      "555.5550 + 555.5550 = 1111.1100 的正确概率: 0.8888, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.10269701 0.89730304]]\n",
      "123.1230 + 123.1230 = 246.2460 的正确概率: 0.8973, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.14482369 0.8551764 ]]\n",
      "7.7777 + 7.7777 = 15.5554 的正确概率: 0.8552, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.3218081 0.6781919]]\n",
      "0.5000 + 0.5000 = 1.0000 的正确概率: 0.6782, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.12247562 0.8775244 ]]\n",
      "444.4440 + 444.4440 = 888.0000 的正确概率: 0.8775, 实际上是: 错误, 检验结果是否正确: 错误\n",
      "原始输出: [[0.12065372 0.87934625]]\n",
      "777.7770 + 777.7770 = 1555.5550 的正确概率: 0.8793, 实际上是: 错误, 检验结果是否正确: 错误\n",
      "原始输出: [[0.61267084 0.3873292 ]]\n",
      "3.3333 + 3.3333 = 6.6000 的正确概率: 0.3873, 实际上是: 错误, 检验结果是否正确: 正确\n",
      "原始输出: [[0.5438608 0.4561392]]\n",
      "1.1111 + 1.1111 = 2.2000 的正确概率: 0.4561, 实际上是: 错误, 检验结果是否正确: 正确\n",
      "原始输出: [[0.9921549  0.00784512]]\n",
      "9.9999 + 9.9999 = 19.0000 的正确概率: 0.0078, 实际上是: 错误, 检验结果是否正确: 正确\n",
      "原始输出: [[0.00243126 0.99756867]]\n",
      "9999.9900 + 0.0100 = 10000.0000 的正确概率: 0.9976, 实际上是: 正确, 检验结果是否正确: 正确\n",
      "原始输出: [[0.5845997  0.41540036]]\n",
      "0.0001 + 999.9999 = 1000.0000 的正确概率: 0.4154, 实际上是: 正确, 检验结果是否正确: 错误\n",
      "原始输出: [[0.57350993 0.42649004]]\n",
      "0.1234 + 987.8766 = 988.0000 的正确概率: 0.4265, 实际上是: 正确, 检验结果是否正确: 错误\n",
      "原始输出: [[0.71963614 0.28036383]]\n",
      "1.0000 + 9999.0000 = 10000.0000 的正确概率: 0.2804, 实际上是: 正确, 检验结果是否正确: 错误\n",
      "原始输出: [[0.68972296 0.31027704]]\n",
      "0.0001 + 0.9999 = 1.0000 的正确概率: 0.3103, 实际上是: 正确, 检验结果是否正确: 错误\n",
      "原始输出: [[0.00246046 0.9975395 ]]\n",
      "9876.5400 + 0.0100 = 9876.5400 的正确概率: 0.9975, 实际上是: 错误, 检验结果是否正确: 错误\n",
      "原始输出: [[0.28034928 0.7196507 ]]\n",
      "0.0001 + 100.0000 = 100.0002 的正确概率: 0.7197, 实际上是: 错误, 检验结果是否正确: 错误\n",
      "原始输出: [[0.01728441 0.9827156 ]]\n",
      "1234.5600 + 0.0010 = 1234.5600 的正确概率: 0.9827, 实际上是: 错误, 检验结果是否正确: 错误\n",
      "原始输出: [[0.7662534  0.23374662]]\n",
      "0.0001 + 9999.9998 = 9999.0000 的正确概率: 0.2337, 实际上是: 错误, 检验结果是否正确: 正确\n",
      "原始输出: [[0.75953513 0.24046487]]\n",
      "0.1111 + 9999.8888 = 9999.9000 的正确概率: 0.2405, 实际上是: 错误, 检验结果是否正确: 正确\n",
      "原始输出: [[0.30360958 0.69639045]]\n",
      "原始输出: [[0.06405322 0.93594676]]\n",
      "原始输出: [[0.32635722 0.67364275]]\n",
      "原始输出: [[0.07651028 0.92348975]]\n",
      "原始输出: [[0.16146506 0.83853495]]\n",
      "原始输出: [[0.90496624 0.09503371]]\n",
      "原始输出: [[0.1937114 0.8062886]]\n",
      "原始输出: [[0.940557   0.05944296]]\n",
      "原始输出: [[0.57528    0.42472002]]\n",
      "原始输出: [[0.80311286 0.19688709]]\n",
      "原始输出: [[0.07178336 0.9282167 ]]\n",
      "原始输出: [[0.10883652 0.89116347]]\n",
      "原始输出: [[0.2136742  0.78632575]]\n",
      "原始输出: [[0.00243126 0.99756867]]\n",
      "原始输出: [[0.07932483 0.9206752 ]]\n",
      "原始输出: [[0.8206536  0.17934638]]\n",
      "原始输出: [[0.24290387 0.7570961 ]]\n",
      "原始输出: [[0.11504443 0.8849556 ]]\n",
      "原始输出: [[0.45011565 0.5498844 ]]\n",
      "原始输出: [[9.9999988e-01 1.5307424e-07]]\n",
      "原始输出: [[0.16216205 0.837838  ]]\n",
      "原始输出: [[0.7369649  0.26303515]]\n",
      "原始输出: [[0.11988896 0.8801111 ]]\n",
      "原始输出: [[0.08979823 0.9102018 ]]\n",
      "原始输出: [[0.09880769 0.9011923 ]]\n",
      "原始输出: [[0.07667679 0.92332315]]\n",
      "原始输出: [[0.5348556  0.46514437]]\n",
      "原始输出: [[0.13565497 0.8643451 ]]\n",
      "原始输出: [[0.9980895  0.00191051]]\n",
      "原始输出: [[0.18547806 0.8145219 ]]\n",
      "原始输出: [[0.1290495 0.8709505]]\n",
      "原始输出: [[0.11123041 0.88876957]]\n",
      "原始输出: [[0.10269701 0.89730304]]\n",
      "原始输出: [[0.14482369 0.8551764 ]]\n",
      "原始输出: [[0.3218081 0.6781919]]\n",
      "原始输出: [[0.12247562 0.8775244 ]]\n",
      "原始输出: [[0.12065372 0.87934625]]\n",
      "原始输出: [[0.61267084 0.3873292 ]]\n",
      "原始输出: [[0.5438608 0.4561392]]\n",
      "原始输出: [[0.9921549  0.00784512]]\n",
      "原始输出: [[0.00243126 0.99756867]]\n",
      "原始输出: [[0.5845997  0.41540036]]\n",
      "原始输出: [[0.57350993 0.42649004]]\n",
      "原始输出: [[0.71963614 0.28036383]]\n",
      "原始输出: [[0.68972296 0.31027704]]\n",
      "原始输出: [[0.00246046 0.9975395 ]]\n",
      "原始输出: [[0.28034928 0.7196507 ]]\n",
      "原始输出: [[0.01728441 0.9827156 ]]\n",
      "原始输出: [[0.7662534  0.23374662]]\n",
      "原始输出: [[0.75953513 0.24046487]]\n",
      "\n",
      "检验结果统计:\n",
      "总样本数: 50\n",
      "检验结果与实际结果相符的次数: 33\n",
      "检验准确率: 66.00%\n"
     ]
    }
   ],
   "source": [
    "model = AdditionNet().to(device)\n",
    "model.load_state_dict(torch.load(\"model_final.pt\"))\n",
    "decimal_examples = [\n",
    "    # 1-10：简单小数加法（正确结果）\n",
    "    (12.34, 56.78, 69.12),       # 正确\n",
    "    (123.45, 678.90, 802.35),    # 正确\n",
    "    (0.75, 0.25, 1.00),          # 正确\n",
    "    (9.99, 1.01, 11.00),         # 正确\n",
    "    (45.67, 32.33, 78.00),       # 正确\n",
    "    \n",
    "    # 11-20：简单小数加法（错误结果）\n",
    "    (22.22, 33.33, 56.66),       # 错误（正确应为55.55）\n",
    "    (88.88, 11.11, 101.00),      # 错误（正确应为99.99）\n",
    "    (7.50, 2.50, 9.50),          # 错误（正确应为10.00）\n",
    "    (45.50, 45.50, 90.00),       # 错误（正确应为91.00）\n",
    "    (67.89, 12.10, 81.99),       # 错误（正确应为79.99）\n",
    "    \n",
    "    # 21-30：大数加法（正确结果）\n",
    "    (1234.56, 7890.12, 9124.68), # 正确\n",
    "    (8765.43, 1234.57, 10000.00),# 正确\n",
    "    (5432.10, 5432.10, 10864.20),# 正确\n",
    "    (9999.99, 0.01, 10000.00),   # 正确\n",
    "    (4567.89, 3210.11, 7778.00), # 正确\n",
    "    \n",
    "    # 31-40：大数加法（错误结果）\n",
    "    (2345.67, 8901.23, 11000.00),# 错误（正确应为11246.90）\n",
    "    (7777.77, 2222.22, 9999.00), # 错误（正确应为9999.99）\n",
    "    (4444.44, 5555.55, 9999.00), # 错误（正确应为9999.99）\n",
    "    (6789.01, 2345.67, 9035.68), # 错误（正确应为9134.68）\n",
    "    (5000.00, 5000.00, 9000.00), # 错误（正确应为10000.00）\n",
    "    \n",
    "    # 41-50：更复杂的小数加法（正确结果）\n",
    "    (123.456, 876.544, 1000.000),# 正确\n",
    "    (0.1234, 0.8766, 1.0000),    # 正确\n",
    "    (7.5432, 2.4568, 10.0000),   # 正确\n",
    "    (99.9999, 0.0001, 100.0000), # 正确\n",
    "    (567.123, 432.877, 1000.000),# 正确\n",
    "    \n",
    "    # 51-60：更复杂的小数加法（错误结果）\n",
    "    (456.789, 543.211, 999.999), # 错误（正确应为1000.000）\n",
    "    (1.2345, 2.3456, 3.5000),    # 错误（正确应为3.5801）\n",
    "    (6.7890, 3.2109, 9.9998),    # 错误（正确应为9.9999）\n",
    "    (0.0123, 0.0456, 0.0580),    # 错误（正确应为0.0579）\n",
    "    (7.1234, 8.9876, 16.0000),   # 错误（正确应为16.1110）\n",
    "    \n",
    "    # 61-70：接近相等的数加法（正确结果）\n",
    "    (999.999, 999.999, 1999.998),# 正确\n",
    "    (555.555, 555.555, 1111.110),# 正确\n",
    "    (123.123, 123.123, 246.246), # 正确\n",
    "    (7.7777, 7.7777, 15.5554),   # 正确\n",
    "    (0.5000, 0.5000, 1.0000),    # 正确\n",
    "    \n",
    "    # 71-80：接近相等的数加法（错误结果）\n",
    "    (444.444, 444.444, 888.000), # 错误（正确应为888.888）\n",
    "    (777.777, 777.777, 1555.555),# 错误（正确应为1555.554）\n",
    "    (3.3333, 3.3333, 6.6000),    # 错误（正确应为6.6666）\n",
    "    (1.1111, 1.1111, 2.2000),    # 错误（正确应为2.2222）\n",
    "    (9.9999, 9.9999, 19.0000),   # 错误（正确应为19.9998）\n",
    "    \n",
    "    # 81-90：一个数很大一个数很小加法（正确结果）\n",
    "    (9999.99, 0.01, 10000.00),   # 正确\n",
    "    (0.0001, 999.9999, 1000.0000),# 正确\n",
    "    (0.1234, 987.8766, 988.0000),# 正确\n",
    "    (1.0000, 9999.0000, 10000.0000),# 正确\n",
    "    (0.0001, 0.9999, 1.0000),    # 正确\n",
    "    \n",
    "    # 91-100：一个数很大一个数很小加法（错误结果）\n",
    "    (9876.54, 0.01, 9876.54),    # 错误（正确应为9876.55）\n",
    "    (0.0001, 100.0000, 100.0002),# 错误（正确应为100.0001）\n",
    "    (1234.56, 0.001, 1234.56),   # 错误（正确应为1234.561）\n",
    "    (0.0001, 9999.9998, 9999.0000),# 错误（正确应为9999.9999）\n",
    "    (0.1111, 9999.8888, 9999.9000)# 错误（正确应为9999.9999）\n",
    "]\n",
    "\n",
    "for a, b, c in decimal_examples:\n",
    "    prob = check_addition(a, b, c)\n",
    "    correct = abs((a + b) - c) < 1e-5\n",
    "    print(f'{a:.4f} + {b:.4f} = {c:.4f} 的正确概率: {prob:.4f}, 实际上是: {\"正确\" if correct else \"错误\"}, 检验结果是否正确: {\"正确\" if (correct and prob > 0.5) or (not correct and prob < 0.5) else \"错误\"}')\n",
    "# 统计检验结果与实际结果相符的次数与百分比\n",
    "total_examples = len(decimal_examples)\n",
    "correct_predictions = 0\n",
    "\n",
    "for a, b, c in decimal_examples:\n",
    "    prob = check_addition(a, b, c)\n",
    "    correct = abs((a + b) - c) < 1e-5\n",
    "    prediction_correct = (correct and prob > 0.5) or (not correct and prob < 0.5)\n",
    "    \n",
    "    if prediction_correct:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_examples * 100\n",
    "print(f\"\\n检验结果统计:\")\n",
    "print(f\"总样本数: {total_examples}\")\n",
    "print(f\"检验结果与实际结果相符的次数: {correct_predictions}\")\n",
    "print(f\"检验准确率: {accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[np.float64(0.227670248696953),\n",
       " np.float64(0.2890648263178878),\n",
       " np.float64(0.3608488067145301),\n",
       " np.float64(0.45815690999132613),\n",
       " np.float64(0.455340497393906),\n",
       " np.float64(0.45815690999132613)]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_data_log(1.111,2.2,3.3,max_value=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs((1234.56+0.001)-1234.56) < 1e-5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
