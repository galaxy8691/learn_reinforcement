{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gymnasium\n!pip install swig\n!pip install gymnasium[box2d]\nfrom IPython import display\n%matplotlib inline\nimport gymnasium as gym\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport time\nimport random","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-08T06:20:23.468938Z","iopub.execute_input":"2024-10-08T06:20:23.469480Z","iopub.status.idle":"2024-10-08T06:20:58.399663Z","shell.execute_reply.started":"2024-10-08T06:20:23.469427Z","shell.execute_reply":"2024-10-08T06:20:58.397435Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gymnasium in /opt/conda/lib/python3.10/site-packages (0.29.0)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (3.0.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (0.0.4)\nRequirement already satisfied: swig in /opt/conda/lib/python3.10/site-packages (4.2.1)\nRequirement already satisfied: gymnasium[box2d] in /opt/conda/lib/python3.10/site-packages (0.29.0)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[box2d]) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[box2d]) (3.0.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium[box2d]) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium[box2d]) (0.0.4)\nRequirement already satisfied: box2d-py==2.3.5 in /opt/conda/lib/python3.10/site-packages (from gymnasium[box2d]) (2.3.5)\nRequirement already satisfied: pygame>=2.1.3 in /opt/conda/lib/python3.10/site-packages (from gymnasium[box2d]) (2.6.1)\nRequirement already satisfied: swig==4.* in /opt/conda/lib/python3.10/site-packages (from gymnasium[box2d]) (4.2.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\ndef display_image(imageArray):\n    display.display(Image.fromarray(np.array(imageArray),'RGB'),clear=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:53:22.404705Z","iopub.execute_input":"2024-10-08T05:53:22.405286Z","iopub.status.idle":"2024-10-08T05:53:22.411733Z","shell.execute_reply.started":"2024-10-08T05:53:22.405227Z","shell.execute_reply":"2024-10-08T05:53:22.410500Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from collections import namedtuple\nTransition  = namedtuple('Transition', ('state','action', 'state_next','reward'))","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:34:43.815682Z","iopub.execute_input":"2024-10-08T06:34:43.816121Z","iopub.status.idle":"2024-10-08T06:34:43.821833Z","shell.execute_reply.started":"2024-10-08T06:34:43.816076Z","shell.execute_reply":"2024-10-08T06:34:43.820600Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"model = nn.Sequential()\nmodel.add_module('fc1',nn.Linear(4,32))\nmodel.add_module('relu1',nn.ReLU())\nmodel.add_module('fc2', nn.Linear(32,32))\nmodel.add_module('relu2', nn.ReLU())\nmodel.add_module('fc2', nn.Linear(32,2))\na = []\nmemory=[]\nmemory.append(Transition([1,2,3,4],[0],[3,4,2,2],1))\nmemory.append(Transition([5,5,7,6],[1],[7,0,2,0],0))\nmemory.append(Transition([6,2,3,4],[1],None,0))\n#memory.append(Transition([6,2,3,4],[1],[3,5,4,2],0))\nbatch = Transition(*zip(*memory))\n\n\n\nstate_batch = torch.Tensor(batch.state)\naction_batch = torch.Tensor(batch.action)\nreward_batch = torch.Tensor(batch.reward)\nnon_final_next_states = torch.Tensor([s for s in batch.state_next if s is not None])\nstate_batch\nmodel.eval()\nmodel(state_batch).gather(1,action_batch.long())\nmask = torch.ByteTensor(tuple(map(lambda s: s is not None, batch.state_next))) != 0\nnsv = torch.zeros(4)\nnsv[mask] = model(non_final_next_states).max(1)[0].detach()\nreplay()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T07:43:11.527473Z","iopub.execute_input":"2024-10-08T07:43:11.527924Z","iopub.status.idle":"2024-10-08T07:43:11.575887Z","shell.execute_reply.started":"2024-10-08T07:43:11.527876Z","shell.execute_reply":"2024-10-08T07:43:11.574370Z"},"trusted":true},"execution_count":117,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[117], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mByteTensor(\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m s: s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, batch\u001b[38;5;241m.\u001b[39mstate_next))) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     25\u001b[0m nsv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mnsv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m model(non_final_next_states)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     27\u001b[0m replay()\n","\u001b[0;31mIndexError\u001b[0m: The shape of the mask [3] at index 0 does not match the shape of the indexed tensor [4] at index 0"],"ename":"IndexError","evalue":"The shape of the mask [3] at index 0 does not match the shape of the indexed tensor [4] at index 0","output_type":"error"}]},{"cell_type":"code","source":"NUM_DIZITIZED = 6\nMAX_STEP = 500\nNUM_EPISODE = 500\nCAPACITY = 10000\nBATCH_SIZE = 3\nGAMMA = 0.99\nETA = 0.1\nmemory = []\nindex = 0\n\nmodel = nn.Sequential()\nmodel.add_module('fc1',nn.Linear(4,32))\nmodel.add_module('relu1',nn.ReLU())\nmodel.add_module('fc2', nn.Linear(32,32))\nmodel.add_module('relu2', nn.ReLU())\nmodel.add_module('fc2', nn.Linear(32,2))\n\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\ndef push(state, action, state_next, reward):\n    if len(memory) < CAPACITY:\n        memory.append(None)\n    memory[index] = Transition(state,action,state_next, reward)\n    index = (index + 1) % CAPACITY\n\ndef sample(batch_size):\n    return random.sample(memory,batch_size)\n\ndef replay():\n    if len(memory) < CAPACITY:\n        return\n    transition = sample(BATCH_SIZE)\n    batch = Transition(*zip(*memory))\n    state_batch = torch.Tensor(batch.state)\n    action_batch = torch.Tensor(batch.action)\n    reward_batch = torch.Tensor(batch.reward)\n    non_final_next_states = torch.Tensor([s for s in batch.state_next if s is not None])\n    model.eval()\n    state_action_value = model(state_batch).gather(1,action_batch.long())\n    mask = torch.ByteTensor(tuple(map(lambda s: s is not None, batch.state_next))) != 0\n    nsv = torch.zeros(BATCH_SIZE)\n    nsv[mask] = model(non_final_next_states).max(1)[0].detach()\n    expected_state_action_values = reward_batch + GAMMA * nsv\n    model.train()\n    loss = F.smooth_l1_loss(state_action_value, expected_state_action_values.unsqueeze(1))\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    \ndef reset(env):\n      observation = env.reset()[0]\n      s_a_history = [[observation, np.nan]]\n      pi = [0.5,0.5]\n      return observation, s_a_history, pi\n\ndef bins(clip_min, clip_max, num):\n    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n\ndef digitize_state(observation):\n    cart_pos, cart_v, pole_angle, pole_v = observation\n    digitized = [\n        np.digitize(cart_pos, bins=bins(-2.4, 2.4, NUM_DIZITIZED)),\n        np.digitize(cart_v, bins=bins(-3.0, 3.0, NUM_DIZITIZED)),\n        np.digitize(pole_angle, bins=bins(-0.5, 0.5, NUM_DIZITIZED)),\n        np.digitize(pole_v, bins=bins(-2.0, 2.0, NUM_DIZITIZED))\n    ]\n    return sum([x * (NUM_DIZITIZED**i) for i, x in enumerate(digitized)])\n\ndef get_action(s,Q,epsilon, pi=[0.5,0.5]):\n    state = digitize_state(s)\n    if np.random.rand() < epsilon:\n        action = np.random.choice([0,1])\n    else:\n        action = np.argmax(Q[state])\n    return action\n\ndef Q_learning(s, a, r, s_next, Q, eta, gamma, is_terminated):\n    s = digitize_state(s)\n    s_next = digitize_state(s_next)\n    if is_terminated:\n        Q[s, a] = Q[s, a] + eta * (r- Q[s, a])\n    else:\n        Q[s, a] = Q[s, a] + eta * (r + gamma * np.nanmax(Q[s_next, :]) - Q[s, a])\n    return Q\n\ndef run_once(env, s_a_history,initial_observation, Q, epsilon, eta, gamma, pi, show=False):\n    observation = initial_observation\n    s = observation\n    a = get_action(s, Q, epsilon, pi)\n    a_next = a\n    count = 0;\n    while True:\n        is_terminated = False\n        if show:\n            time.sleep(0.01)\n            img = env.render()\n            display_image(img)\n        a = a_next\n        s_a_history[-1][1] = a\n        s_next, reward, terminated, truncated, info = env.step(a)\n        s_a_history.append([s_next, np.nan])\n        if count > 500:\n            r = 1\n            a_next = np.nan\n            is_terminated = True\n        elif count < 500 and terminated:\n            r = -1\n            a_next = np.nan\n            is_terminated = True\n        else:\n            r = 0\n            a_next = get_action(s_next, Q, epsilon, pi)\n        Q = Q_learning(s, a, r, s_next, Q, eta, gamma, is_terminated)\n        if is_terminated:\n            if show:\n                time.sleep(0.01)\n                img = env.render()\n                display_image(img)\n            break\n        else:\n            s = s_next\n        count += 1\n    return [s_a_history, Q]","metadata":{"execution":{"iopub.status.busy":"2024-10-08T07:43:05.796137Z","iopub.execute_input":"2024-10-08T07:43:05.796588Z","iopub.status.idle":"2024-10-08T07:43:05.825000Z","shell.execute_reply.started":"2024-10-08T07:43:05.796543Z","shell.execute_reply":"2024-10-08T07:43:05.823901Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"env = gym.make('CartPole-v1', render_mode='rgb_array')\neta = 0.1\ngamma = 0.9\nepsilon = 0.5\nQ = np.random.rand(NUM_DIZITIZED**4,2)\nv = np.nanmax(Q, axis=1)\nis_continue = True\nepisode = 1\nenv.reset()\nwhile is_continue:\n    observation, s_a_history, pi = reset(env)\n    #print('episode: ' + str(episode))\n    epsilon = epsilon / 2\n    [s_a_history, Q] = run_once(env, s_a_history, observation, Q, epsilon, eta, gamma, pi,show=False)\n    new_v = np.nanmax(Q, axis=1)\n    #print(np.sum(np.abs(new_v - v)))\n    v = new_v\n    episode = episode + 1\n    if episode > 1000:\n        break\nprint(np.sum(np.abs(new_v - v)))\nenv.close()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T05:57:21.948192Z","iopub.execute_input":"2024-10-08T05:57:21.948645Z","iopub.status.idle":"2024-10-08T05:58:08.854997Z","shell.execute_reply.started":"2024-10-08T05:57:21.948601Z","shell.execute_reply":"2024-10-08T05:58:08.853789Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"env = gym.make('CartPole-v1', render_mode='rgb_array')\nenv.reset()\nobservation, s_a_history, pi = reset(env)\n#run_once(env, s_a_history, observation, Q, 0, eta, gamma, pi,show=True)\nenv.observation_space.shape","metadata":{"execution":{"iopub.status.busy":"2024-10-08T06:21:28.780440Z","iopub.execute_input":"2024-10-08T06:21:28.780907Z","iopub.status.idle":"2024-10-08T06:21:28.791620Z","shell.execute_reply.started":"2024-10-08T06:21:28.780860Z","shell.execute_reply":"2024-10-08T06:21:28.790200Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"(4,)"},"metadata":{}}]}]}